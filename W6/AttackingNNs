Adversarial sample - given to a trained model to make it malfunction (on purpose); E.G.: Adversarial Patches

Epsilon Noising Method -> adding random noise to each pixel of an image, with aplitude [-epsilon; epsilon]

Good Adversarial Sample:
  1) Results into model malfunctioning 
  2) Sample still appears plausible to a human person 

Plausibility -> measured by distance betweend original input & adversarial sample (distance constraint - chosen arbitrarily)

Targeted vs Untargeted:
  Untargeted attacks will result into any misclassification
  Targeted attacks will result into misclassification as a specific class

Black-Box vs White-Box:
  Black-Box attack does not exploit properties of the model
  White-Box attack attempts to exploit properties of the model

One-Shot vs Iterated:
  One-Shot attack - single attack per sample
  Iterated attack - produce attack sample, adjusts until model malfunctions or iteration limit reached

